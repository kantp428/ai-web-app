import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Device (CPU/GPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Model ID ‡∏ö‡∏ô Hugging Face Hub
MODEL_ID = "mrktp/text-mini-gpt2-finetuned"

# ‡πÇ‡∏´‡∏•‡∏î model + tokenizer ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏¥‡πà‡∏° run
print("üîÑ Loading model... please wait.")
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(MODEL_ID)
model.to(device)
model.eval()
print("‚úÖ Model loaded successfully.")

def predict_with_model(text: str, top_k: int = 8):
    """‡∏£‡∏±‡∏ö input text ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏∑‡∏ô top-k predicted next tokens"""

    # tokenize input
    inputs = tokenizer(text, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits

    # logit ‡∏Ç‡∏≠‡∏á token ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
    next_token_logits = logits[0, -1]

    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å top-k token id
    top_token_ids = torch.topk(next_token_logits, top_k).indices.tolist()

    # decode ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô string ‡πÅ‡∏•‡∏∞ strip whitespace
    top_tokens = [
        tokenizer.decode([tid], skip_special_tokens=True).strip()
        for tid in top_token_ids
        if tokenizer.decode([tid], skip_special_tokens=True).strip()  # ‡∏Å‡∏±‡∏ô token ‡∏ß‡πà‡∏≤‡∏á
    ]

    return top_tokens
